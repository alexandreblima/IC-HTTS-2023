{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"id":"4rWTJ0O2OAkU","colab":{"base_uri":"https://localhost:8080/","height":404},"executionInfo":{"status":"error","timestamp":1693423133350,"user_tz":180,"elapsed":801,"user":{"displayName":"Alexandre B. de Lima","userId":"11085624188146921102"}},"outputId":"9a7b922c-920b-4ce0-8a96-25e0ed4b6c59"},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-46abc7b88baf>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m   \"\"\"\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    161\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     result = _output.eval_js(\n\u001b[1;32m    165\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"]}],"source":["# State-of-Charge (SOC) estimation of Panasonic 18650PF Li-ion Battery\n","# With Hidden Layers\n","# Data driven estimation using machine learning - neural nets\n","# data: https://data.mendeley.com/datasets/wykht8y7tg/1\n","# NN - 25degC\n","# contributor:\n","# Author: Heloisa\n","# FCET - PUC/SP\n","\n","# Date: 22/08/2023\n","\n","import tensorflow as tf\n","from keras.models import Model#, Sequential\n","from keras import layers#, regularizers, models\n","from keras import Input\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from google.colab import files\n","uploaded = files.upload()\n","\n","import scipy.io\n","train_data_dic = scipy.io.loadmat('train_data.mat')\n","\n","train_data = train_data_dic['train_data']\n","\n","test_data_dic = scipy.io.loadmat('test_data.mat')\n","test_data = test_data_dic['test_data']\n","\n","train_label_dic = scipy.io.loadmat('train_label.mat')\n","train_targets = train_label_dic['SOC_label_train']\n","\n","test_label_dic = scipy.io.loadmat('test_label.mat')\n","test_targets = test_label_dic['SOC_label_test']\n","\n","# This is the Neural Net Model\n","def build_model():\n","    # modelo equivalente usando a API funcional\n","    input_tensor = Input(shape=(train_data.shape[1],))\n","    x = layers.Dense(n_hidden, activation='relu')(input_tensor) # hidden layer 1\n","    x = layers.Dense(n_hidden, activation='relu')(x) # hidden layer 2\n","    x = layers.Dense(n_hidden, activation='relu')(x) # hidden layer 3\n","    x = layers.Dense(n_hidden, activation='relu')(x) # hidden layer 4\n","    output_tensor = layers.Dense(1)(x)\n","    model = Model(input_tensor, output_tensor)\n","    #model.compile(tf.keras.optimizers.SGD(learning_rate=lr), loss='mse', metrics=['mae'])\n","    #model.compile(tf.keras.optimizers.RMSprop(learning_rate=lr), loss='mse', metrics=['mae'])\n","    #model.compile(tf.keras.optimizers.Adamax(learning_rate=lr), loss='mse', metrics=['mae'])\n","    model.compile(tf.keras.optimizers.Adam(learning_rate=lr), loss='mse', metrics=['mae'])\n","    #model.compile(tf.keras.optimizers.Adadelta(learning_rate=lr), loss='mse', metrics=['mae'])\n","    #model.compile(tf.keras.optimizers.Adagrad(learning_rate=lr), loss='mse', metrics=['mae'])\n","    #model.compile(tf.keras.optimizers.Nadam(learning_rate=lr), loss='mse', metrics=['mae'])\n","    #model.compile(tf.keras.optimizers.Ftrl(learning_rate=lr), loss='mse', metrics=['mae'])\n","    return model\n","\n","# Regularization with K-Fold\n","k = 4 # number of folds for cross-validation\n","num_val_samples = len(train_data) // k\n","num_epochs = 100 # let default value be 50\n","num_epochs_train = 50 # let default value be 50\n","all_scores = []\n","BATCH_SIZE = 128\n","n_hidden = 256 # units\n","# Step size\n","lr = 0.001\n","# lr = 0.0001\n","\n","all_loss_histories = []\n","all_mae_histories = []\n","all_val_loss_histories = []\n","all_val_mae_histories = []\n","\n","from keras import backend as K\n","# Some memory clean-up\n","K.clear_session()\n","# K-fold validation\n","\n","for i in range(k):\n","    print('processing fold #', i)\n","    # Prepare the validation data: data from partition # k\n","    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n","    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n","\n","    # Prepare the training data: data from all other partitions\n","    partial_train_data = np.concatenate([train_data[:i * num_val_samples],train_data[(i + 1) * num_val_samples:]], axis=0)\n","    partial_train_targets = np.concatenate([train_targets[:i * num_val_samples],train_targets[(i + 1) * num_val_samples:]], axis=0)\n","\n","    # Build the Keras model (already compiled)\n","    model = build_model()\n","    # Train the model (in silent mode, verbose=0)\n","    treino = model.fit(partial_train_data, partial_train_targets, validation_data=(val_data, val_targets),\n","                       epochs=num_epochs, batch_size=BATCH_SIZE, verbose=1)\n","    # Keep a record of how well the model did at each epoch\n","    # save the per-epoch validation score log:\n","    loss_history = treino.history['loss']\n","    mae_history = treino.history['mae']\n","    val_loss_history = treino.history['val_loss']\n","    val_mae_history = treino.history['val_mae']\n","\n","    all_loss_histories.append(loss_history)\n","    all_mae_histories.append(mae_history)\n","    all_val_loss_histories.append(val_loss_history)\n","    all_val_mae_histories.append(val_mae_history)\n","\n","# We can then compute the average of the per-epoch MAE scores for all folds:\n","average_loss_history = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n","average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n","average_val_loss_history = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n","average_val_mae_history = [np.mean([x[i] for x in all_val_mae_histories]) for i in range(num_epochs)]\n","\n","font = {'family': 'serif',\n","        'color':  'darkred',\n","        'weight': 'normal',\n","        'size': 16,\n","        }\n","\n","# plt.clf()\n","\n","plt.figure()\n","plt.plot(range(1, len(average_loss_history) + 1), 20*np.log10(average_loss_history), 'b',label='Loss function (dB)')\n","plt.plot(range(1, len(average_val_loss_history) + 1), 20*np.log10(average_val_loss_history), 'r', label='Validation loss function (dB)')\n","plt.title('Validation Phase - Learning curves', fontdict=font)\n","plt.xlabel('Epoch', fontdict=font)\n","plt.ylabel('Loss function', fontdict=font)\n","plt.legend()\n","plt.show()\n","\n","plt.figure()\n","plt.plot(range(1, len(average_mae_history) + 1), 20*np.log10(average_mae_history), 'b', label='MAE (dB)')\n","plt.plot(range(1, len(average_val_mae_history) + 1), 20*np.log10(average_val_mae_history), 'r', label='Validation MAE (dB)' )\n","plt.title('Validation Phase - Learning curves', fontdict=font)\n","plt.xlabel('Epoch', fontdict=font)\n","plt.ylabel('Error - MAE', fontdict=font)\n","plt.legend()\n","\n","plt.show()\n","\n","# Model construction ... THIS IS IMPORTANT !!!\n","model = build_model()\n","# Model summary\n","model.summary()\n","\n","# Train the model on the entire training data - REAL training phase !!!\n","real_train = model.fit(train_data, train_targets, epochs=num_epochs_train, batch_size=128, verbose=1)\n","\n","# Keep a record of how well the model did at each epoch at REAL training phase\n","# save the per-epoch validation score log:\n","loss_hist = real_train.history['loss']\n","mae_hist = real_train.history['mae']\n","\n","plt.figure()\n","\n","plt.plot(range(1, len(loss_hist) + 1), 20*np.log10(loss_hist), 'r', label='loss function (dB)')\n","plt.plot(range(1, len(mae_hist) + 1),  20*np.log10(mae_hist), 'b', label='MAE (dB)')\n","plt.title('Training Phase - Learning curves', fontdict=font)\n","plt.xlabel('Epoch', fontdict=font)\n","plt.ylabel('MAE/Loss', fontdict=font)\n","plt.legend()\n","\n","plt.show()\n","\n","# Evaluate the model on the test data\n","test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\n","\n","# Performance on the test data\n","print('Test MAE score', test_mae_score)\n","print('Test MSE score', test_mse_score)\n","\n","test_pred = model.predict(test_data)\n","\n","plt.figure()\n","\n","plt.plot(test_targets, test_pred, 'b', label='SOC')\n","plt.title('SOC: test set vs predictions', fontdict=font)\n","plt.xlabel('SOC', fontdict=font)\n","plt.ylabel('Predicted SOC', fontdict=font)\n","plt.legend()\n","\n","plt.show()\n","\n","# 4 hidden layers - 256 hidden units - batch size = 128 - step size = 0.001\n","# Mean Absolute Error (MAE) and MSE - performance over the TEST SET\n","# Optimizers\n","\n","# Date: 22/08/2023\n","\n","# SGD: MAE = , MSE = 0\n","# Adagrad: MAE = , MSE =\n","# RMSProp: MAE = , MSE =\n","# Adam: MAE = , MSE =\n","\n","# Adadelta: MAE = , MSE =\n","# Adamax:  MAE = , MSE =\n","# Nadam: MAE = , MSE =\n","# Ftlr: MAE = , MSE ="]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1ERQGCjIa5tVqpglxWqToigW0PmT397mG","timestamp":1599363014801}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}